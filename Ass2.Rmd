---
title: "Topic modelling project: "
output: html_document
---

### 0. Preparation: Load packages

```{r, message = FALSE}
defaultW <- getOption("warn") 
options(warn = -1) 
library(tidyverse)
library(tidytext)
library(rtweet)
library(wordcloud2)
library(topicmodels)
library(lubridate)
library(LDAvis)
library(scales)
library(servr)
options(warn = defaultW)
```

## Timeline comparison

### 1 API request
```{r}
api_key <- "kcrkqTWQ6piaQugATqOEsg7ZW"
api_secret_key <- "KwX8hmYLdxpoXRMDv9yK6uluJPCMFOxgOSZFoJSTKTdOwLCKQq"
access_token <- "1051850979661021184-1hynI2jXn7Umy2HP0sZBi4gaDQzuBt"
access_token_secret <- "zfRVgeDqHQUDwn26ojLTzk7QHA59qpQqF17rGQVvi6Q2E"

token <- create_token(app = "UnibgTwitter1082586", consumer_key = api_key, 
                      consumer_secret = api_secret_key, access_token = access_token, 
                      access_secret = access_token_secret, set_renv = FALSE)
```

#### 1.a Get the timelines
```{r}
timeline <- get_timeline(c("745273", "8161232"),
    n = Inf, token = token)
```

### 2. Comparison
```{r}
timeline %>%
    group_by(screen_name) %>%
    ts_plot(by = "month") + ggplot2::labs(title = "Tweets by business men",
    subtitle = "Naval Ravikant vs Richard Branson") + scale_colour_manual(values = c("steelblue",
    "skyblue")) + theme_bw()
```

#### 2.a Word usage comparison
```{r}
timeline_f <- timeline %>%
    filter(created_at >= as.Date("2020-04-21"))

text <- timeline_f %>%
    select(status_id, text, screen_name)

tidy_tweets <- text %>%
    unnest_tweets(word, text) %>%
    anti_join(stop_words)

library(scales)


tidy_tweets %>%
    group_by(screen_name) %>%
    count(word, sort = T) %>%
    mutate(prop = n/sum(n)) %>%
    select(screen_name, word, prop) %>%
    pivot_wider(names_from = screen_name, values_from = prop) %>%
    arrange(naval, richardbranson) %>%
    ggplot(aes(naval, richardbranson)) + geom_jitter(alpha = 0.5,
    size = 2.5, width = 0.25, height = 0.25, colour = "steelblue") +
    geom_text(aes(label = word), check_overlap = T, vjust = 0) +
    scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) +
    geom_abline(color = "red") + theme_bw()
```

#### 2.a Word usage comparison - with data cleaned
```{r}
new_stop <- tibble(word = c("dont", "youre", "im", "ive", "isnt", "whats", "youre", "theyre", "theres", "thats", "arent", "cant", "couldnt", "didnt", "doesnt", "hadnt", "hasnt", "havent", "hed", "hes", "youll", "itll")) 
new_stop$lexicon <- "custom"
custom_stop <- bind_rows(stop_words, new_stop)

tidy_tweets <- text %>%
    unnest_tweets(word, text) %>%
    anti_join(custom_stop)

tidy_tweets <- tidy_tweets %>% filter(!str_detect(word,"[^a-z A-Z 0-9]"))
tidy_tweets <- tidy_tweets %>% filter(!str_detect(word, "[:digit:]"))
tidy_tweets <- tidy_tweets %>% filter(!str_detect(word, "^@.*"))
tidy_tweets <- tidy_tweets %>% filter(!str_detect(word, "^https.*"))


tidy_tweets %>%
    group_by(screen_name) %>%
    count(word, sort = T) %>%
    mutate(prop = n/sum(n)) %>%
    select(screen_name, word, prop) %>%
    pivot_wider(names_from = screen_name, values_from = prop) %>%
    arrange(naval, richardbranson) %>%
    ggplot(aes(naval, richardbranson)) + geom_jitter(alpha = 0.5,
    size = 2.5, width = 0.25, height = 0.25, colour = "steelblue") +
    geom_text(aes(label = word), check_overlap = T, vjust = 0) +
    scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) +
    geom_abline(color = "red") + theme_bw()
```

## Naval tweets 

### 1 Overview

#### 1.a Extracting
```{r}
naval_tweets <- timeline %>% filter(screen_name == "naval")
```

#### 1.b how many retweets?
```{r}
naval_tweets %>% filter(is_retweet == TRUE) %>% count()
```

#### 1.c how many quotes?
```{r}
naval_tweets %>% filter(is_quote == TRUE) %>% count()
```

#### 1.d how many replies?
```{r}
naval_tweets %>% filter(!is.na(reply_to_user_id)) %>% count()
naval_tweets %>% filter(!is.na(reply_to_screen_name)) %>% count() #double check
```

#### 1.e max and min date
```{r}
naval_tweets_desc <- naval_tweets %>% arrange(desc(created_at))
str_extract(naval_tweets_desc$created_at[1], "[:digit:]*-[:digit:]*-[:digit:]*") #Max (most recent tweet)

naval_tweets_asc <- naval_tweets %>% arrange(created_at)
str_extract(naval_tweets_asc$created_at[1], "[:digit:]*-[:digit:]*-[:digit:]*") #Min (less recent tweet)
```

### 2. Naval tweets - Topic modelling

#### 2.a Extracting
```{r}
naval_tweets <- naval_tweets %>%
    mutate(id = 1:nrow(naval_tweets))

text <- naval_tweets %>%
    select(text, id)
text
```

#### 2.b Cleaning
```{r}

tidy_text <- text %>%
    unnest_tweets(word, text) %>%
    anti_join(custom_stop) %>%
    count(id, word)
tidy_text <- tidy_text %>% filter(!str_detect(word,"[^a-z A-Z 0-9]"))
tidy_text <- tidy_text %>% filter(!str_detect(word, "[:digit:]"))
tidy_text <- tidy_text %>% filter(!str_detect(word, "^@.*"))
tidy_text <- tidy_text %>% filter(!str_detect(word, "^https.*"))
```

#### 2.c Dtm
```{r}
dtm <- tidy_text %>%
    cast_dtm(id, word, n)
dtm
```

#### 2.d Testing
```{r}
set.seed(123456)
train <- sample(rownames(dtm), nrow(dtm) * 0.75)
dtm_train <- dtm[rownames(dtm) %in% train, ]
dtm_test <- dtm[!rownames(dtm) %in% train, ]

topic <- data.frame(k = c(5, 15, 25, 30, 35), perplexity = NA)

for (i in 1:nrow(topic)) {
    print(topic$k[i])
    m = LDA(dtm_train, method = "Gibbs", k = topic$k[i], control = list(alpha = 0.01,
        seed = 123456))
    topic$perplexity[i] = perplexity(m, dtm_test)
}
```

```{r}
ggplot(topic, aes(x = k, y = perplexity)) + geom_line(col = "steelblue") +
    theme_bw()
```

Different range
```{r}
topic <- data.frame(k = c(25, 30, 35, 40, 45), perplexity = NA)

for (i in 1:nrow(topic)) {
    print(topic$k[i])
    m = LDA(dtm_train, method = "Gibbs", k = topic$k[i], control = list(alpha = 0.01,
        seed = 123456))
    topic$perplexity[i] = perplexity(m, dtm_test)
}
```

```{r}
ggplot(topic, aes(x = k, y = perplexity)) + geom_line(col = "steelblue") +
    theme_bw()
```

```{r}
topic <- data.frame(k = c(21, 22, 23, 24, 25), perplexity = NA)

for (i in 1:nrow(topic)) {
    print(topic$k[i])
    m = LDA(dtm_train, method = "Gibbs", k = topic$k[i], control = list(alpha = 0.01,
        seed = 123456))
    topic$perplexity[i] = perplexity(m, dtm_test)
}
```

```{r}
ggplot(topic, aes(x = k, y = perplexity)) + geom_line(col = "steelblue") +
    theme_bw()
```

#### 2.e Running the model
```{r}
m <- LDA(dtm, method = "Gibbs", k = 25, control = list(alpha = 0.01,
    seed = 123456))
```

```{r}
terms(m, 7)
```

#### 2.g Per-topic per-word probabilities
```{r}
tidy_topics <- tidy(m, matrix = "beta") 

tidy_topics
```

#### 2.h Results plot
```{r}
top_terms <- tidy_topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 7, with_ties = F) %>%
    ungroup() %>%
    arrange(topic, -beta)

top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term)) + geom_col(show.legend = F, fill = "skyblue") +
    facet_wrap(~topic, scales = "free") + scale_y_reordered() +
    theme_bw()
```

#### 2.i Word assignment to each topic
```{r}
word_ass <- augment(m, dtm)
word_ass %>% arrange(desc(count))
```

#### 2.l Topic proportion in documents
```{r}
tidy_gamma <- tidy(m, matrix = "gamma")

#we choose two random topics and sort the gamma values to verify which document has the higher prevalence of the selected topic
tidy_gamma %>% filter(topic == 1) %>% arrange(desc(gamma)) 
tidy_gamma %>% filter(topic == 2) %>% arrange(desc(gamma))
tidy_gamma %>% filter(topic == 15) %>% arrange(desc(gamma))
```

We check also the content of the two tweets with the highest gamma
```{r}
naval_tweets %>% filter(id == 167) %>% select(text)
naval_tweets %>% filter(id == 253) %>% select(text)
naval_tweets %>% filter(id == 617) %>% select(text)
```

#### 2.m Checking how many documents contain a certain topic
```{r}
doc_class <- tidy_gamma %>%
    group_by(document) %>%
    slice_max(gamma) %>%
    ungroup() %>%
    arrange(as.numeric(document))

doc_class <- doc_class %>%
    anti_join(doc_class %>%
        select(document) %>%
        mutate(dup = duplicated(document)) %>%
        filter(dup == "TRUE"))

doc_ass <- naval_tweets %>%
    mutate(id = as.character(id)) %>%
    rename(document = "id") %>%
    inner_join(doc_class)

doc_ass
```

```{r}
doc_ass %>%
  count(topic) %>% ggplot(aes(x = reorder(topic,n), y = n, fill = reorder(factor(topic), n)))+
  geom_bar(stat = 'identity') +
  theme(legend.position = "null") +
  coord_flip()

```

#### 2.n Topics variation over time
```{r}
doc_ass %>%
    mutate(year = year(created_at)) %>%
    group_by(year) %>%
    count(topic)
```

```{r}
t <- doc_ass %>% 
  mutate(date = as.Date(created_at)) %>% 
  group_by(date) %>% count(topic)  

require(reshape2)

t %>% 
    ggplot(aes(date, y = n, color = factor(topic))) +
    geom_line(stat="identity", size = 1) +
    theme(legend.position = "none") +
    facet_wrap(~topic)

t %>% filter(topic == 21) %>% arrange(desc(n))
t %>% filter(topic == 22) %>% arrange(desc(n))
```

#### 2.o Interactive view for topics
```{r}
dtm <- dtm[slam::row_sums(dtm) > 0, ]
phi <- as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]

json <- createJSON(phi = phi, theta = theta, vocab = vocab, doc.length = doc.length,
    term.frequency = term.freq)
serVis(json)
```